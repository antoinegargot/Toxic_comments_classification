{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Classification for Toxic comments online\n",
    "A Keras tensor is a tensor object from the underlying backend (Theano or TensorFlow), which we augment with certain attributes that allow us to build a Keras model just by knowing the inputs and outputs of the model. We use tensorflow backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T15:42:08.741220Z",
     "start_time": "2018-04-20T15:42:05.055341Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define f1 computation measure for the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T15:42:10.632170Z",
     "start_time": "2018-04-20T15:42:10.608903Z"
    }
   },
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T15:42:13.059983Z",
     "start_time": "2018-04-20T15:42:11.927322Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('DATA/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting training and testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T15:42:13.866414Z",
     "start_time": "2018-04-20T15:42:13.815533Z"
    }
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T15:42:14.363386Z",
     "start_time": "2018-04-20T15:42:14.344671Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>122167</th>\n",
       "      <td>8d9074b0cfde5720</td>\n",
       "      <td>The sentence is not NPOV.Ahle Sunnat or Barelv...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90156</th>\n",
       "      <td>f143d62af093c23a</td>\n",
       "      <td>\"so what  is trying to say is that the title s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5298</th>\n",
       "      <td>0e1b3a4f32a2c0a1</td>\n",
       "      <td>\"\\n  Paul,I just saw this or would,for the fin...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42536</th>\n",
       "      <td>7187c1b78358c2d9</td>\n",
       "      <td>no, i can read. and i was under the impression...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92284</th>\n",
       "      <td>f6ba09835b269325</td>\n",
       "      <td>according to your page, your ground forces, wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "122167  8d9074b0cfde5720  The sentence is not NPOV.Ahle Sunnat or Barelv...   \n",
       "90156   f143d62af093c23a  \"so what  is trying to say is that the title s...   \n",
       "5298    0e1b3a4f32a2c0a1  \"\\n  Paul,I just saw this or would,for the fin...   \n",
       "42536   7187c1b78358c2d9  no, i can read. and i was under the impression...   \n",
       "92284   f6ba09835b269325  according to your page, your ground forces, wh...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "122167      0             0        0       0       0              0  \n",
       "90156       0             0        0       0       0              0  \n",
       "5298        0             0        0       0       0              0  \n",
       "42536       0             0        0       0       0              0  \n",
       "92284       0             0        0       0       0              0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T15:42:14.953370Z",
     "start_time": "2018-04-20T15:42:14.935539Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9404</th>\n",
       "      <td>18f4195f676d3ca4</td>\n",
       "      <td>\"\\n\\n Replaceable fair use Image:VmaxBike.JPG ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105852</th>\n",
       "      <td>364437029946420c</td>\n",
       "      <td>REDIRECT Talk:Pine forest stream frog</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148309</th>\n",
       "      <td>4a2592d132a19237</td>\n",
       "      <td>How closely are you checking the sources? You ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>018d35eece5d16b3</td>\n",
       "      <td>Antonov revert \\nIf you didn't like the red-li...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50974</th>\n",
       "      <td>885406d0c31ab241</td>\n",
       "      <td>Sorry about that. ^^; I only wanted to change ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "9404    18f4195f676d3ca4  \"\\n\\n Replaceable fair use Image:VmaxBike.JPG ...   \n",
       "105852  364437029946420c              REDIRECT Talk:Pine forest stream frog   \n",
       "148309  4a2592d132a19237  How closely are you checking the sources? You ...   \n",
       "589     018d35eece5d16b3  Antonov revert \\nIf you didn't like the red-li...   \n",
       "50974   885406d0c31ab241  Sorry about that. ^^; I only wanted to change ...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "9404        0             0        0       0       0              0  \n",
       "105852      0             0        0       0       0              0  \n",
       "148309      0             0        0       0       0              0  \n",
       "589         0             0        0       0       0              0  \n",
       "50974       0             0        0       0       0              0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for presence of any null values, toxic dataset does not have any null values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T15:42:16.069800Z",
     "start_time": "2018-04-20T15:42:15.987265Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(id               False\n",
       " comment_text     False\n",
       " toxic            False\n",
       " severe_toxic     False\n",
       " obscene          False\n",
       " threat           False\n",
       " insult           False\n",
       " identity_hate    False\n",
       " dtype: bool, id               False\n",
       " comment_text     False\n",
       " toxic            False\n",
       " severe_toxic     False\n",
       " obscene          False\n",
       " threat           False\n",
       " insult           False\n",
       " identity_hate    False\n",
       " dtype: bool)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().any(),test.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text data must be encoded as numbers to be used as input or output for deep learning models. keras provides tokenization where we break down our comments into unique words and put the words in a list and index each word. This chain of indexes will be fed to the LSTM So this is what we are going to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T15:42:16.954199Z",
     "start_time": "2018-04-20T15:42:16.951379Z"
    }
   },
   "outputs": [],
   "source": [
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T15:42:17.377444Z",
     "start_time": "2018-04-20T15:42:17.366388Z"
    }
   },
   "outputs": [],
   "source": [
    "y = train[list_classes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T15:42:17.737385Z",
     "start_time": "2018-04-20T15:42:17.734354Z"
    }
   },
   "outputs": [],
   "source": [
    "list_sentences_train = train[\"comment_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T15:42:18.091005Z",
     "start_time": "2018-04-20T15:42:18.087053Z"
    }
   },
   "outputs": [],
   "source": [
    "list_sentences_test = test[\"comment_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some research and experiements, 20,000 features seems to be a good number of feature for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T15:42:42.890465Z",
     "start_time": "2018-04-20T15:42:19.351606Z"
    }
   },
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T15:42:46.108155Z",
     "start_time": "2018-04-20T15:42:46.101528Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 476, 8, 14, 647, 25, 15300, 1176, 24, 11, 551, 5, 11084, 1176]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_tokenized_train[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM requires the data to be of fixed length, that is same number of features, but the comments can be of various lengths and hence the indexing length might vary Hence we go for padding where we set a maxlen allowed to some number(200 in our case) and pad the shorter ones with zeros and cut short the longer ones --> done using pad function\n",
    "We saw the distribution of number of words in sentences in the entire dataset and came up with a convinient number 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T15:42:58.538049Z",
     "start_time": "2018-04-20T15:42:56.739666Z"
    }
   },
   "outputs": [],
   "source": [
    "maxlen = 200\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T15:43:01.285409Z",
     "start_time": "2018-04-20T15:43:01.262497Z"
    }
   },
   "outputs": [],
   "source": [
    "totalNumWords = [len( ) for one_comment in list_tokenized_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below plot to obtain the optimum maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T15:43:04.754401Z",
     "start_time": "2018-04-20T15:43:04.180926Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117cd5390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(totalNumWords,bins = np.arange(0,410,10))#[0,50,100,150,200,250,300,350,400])#,450,500,550,600,650,700,750,800,850,900])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, input function is used to create and define a standalone Input layer that specifies the shape of input data. The input layer takes a shape argument that is a tuple that indicates the dimensionality of the input data. When input data is one-dimensional, the shape must explicitly leave room for the shape of the  mini-batch size used when splitting the data when training the network.  Therefore, the shape tuple is always defined with a hanging last dimension when the input is one-dimensional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T15:46:40.598025Z",
     "start_time": "2018-04-20T15:46:40.517123Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_1:0' shape=(?, 200) dtype=float32>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = Input(shape=(maxlen, ))\n",
    "inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output from the Input() is passed on to the embedding layer where the words are defined in a vector space depending on the surrounding words, the output of the embedding layer is a list of co-ordinates of the words in the vector space. Basically it's a mapping of the original input data into some set of real-valued dimensions,  and the \"position\" of the original input data in those dimensions is organized to improve the task. So, similar words might be put on the same dimensiona nd hence the overall dimensions are reduced drastically. Distance between words are used to determine the relevance of concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T15:46:43.689715Z",
     "start_time": "2018-04-20T15:46:43.663583Z"
    }
   },
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "x = Embedding(max_features, embed_size)(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LSTM, we feed the output of one layer as an input to the next layer. Final output is taken after some number of recursions. We want out LSTM to produce output with dimensions as 60. Taking input from the previous layers, LSTM runs 200 times, passing the coordinates of the words each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T15:46:46.633276Z",
     "start_time": "2018-04-20T15:46:46.460547Z"
    }
   },
   "outputs": [],
   "source": [
    "x = LSTM(60, return_sequences=True,name='lstm_layer')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model x obtained after fillting LSTM will be a 3D model, we need to convert the same into a 2D one, hence we use GlobalMaxPool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T15:46:49.372259Z",
     "start_time": "2018-04-20T15:46:49.365971Z"
    }
   },
   "outputs": [],
   "source": [
    "x = GlobalMaxPool1D()(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get e generalization of the data, we remove some part of the data so that the next layer handles missing data forcefully Dropout(0.1) disables 10% of the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T15:46:52.186207Z",
     "start_time": "2018-04-20T15:46:52.164922Z"
    }
   },
   "outputs": [],
   "source": [
    "x = Dropout(0.1)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of Dropout is given as input to a \"Relu\" for reduced likelihood of vanishing gradient (avoid a neural to quicky die). Dimension of the output is set to 50 Again a Dropout of 10% is achieved and the output is now given to a sigmoid function. Sigmoid function produces output between 0 and 1, hence we achive a binary classification for each of the 6 labels;."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T15:46:54.971158Z",
     "start_time": "2018-04-20T15:46:54.949935Z"
    }
   },
   "outputs": [],
   "source": [
    "x = Dense(50, activation=\"relu\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T15:46:57.754198Z",
     "start_time": "2018-04-20T15:46:57.736145Z"
    }
   },
   "outputs": [],
   "source": [
    "x = Dropout(0.1)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T15:47:00.624063Z",
     "start_time": "2018-04-20T15:47:00.606117Z"
    }
   },
   "outputs": [],
   "source": [
    "x = Dense(6, activation=\"sigmoid\")(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T17:25:57.906059Z",
     "start_time": "2018-04-15T17:25:57.900482Z"
    }
   },
   "source": [
    "Using Adam optimization algorithm that can used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data using Root Mean Square Propagation and Adaptive Gradient Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T15:47:03.601183Z",
     "start_time": "2018-04-20T15:47:03.535087Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy', f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T15:53:02.209818Z",
     "start_time": "2018-04-20T15:53:02.195669Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 200, 128)          2560000   \n",
      "_________________________________________________________________\n",
      "lstm_layer (LSTM)            (None, 200, 60)           45360     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                3050      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 2,608,716\n",
      "Trainable params: 2,608,716\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll feed in a list of 32 padded, indexed sentence for each batch and split 10% of the data as a validation set. This validation set will be used to assess whether the model has overfitted, for each batch.  The model will also run for 2 epochs which is enough regarding the algorthm and the amount of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We catually should consider the total training size/batch size, that many number of batches pass through our algorithm  in each epoch. Typically, you'll split your test set into small batches for the network to learn from, and make the training go step by step through your number of layers, applying gradient-descent all the way down. All these small steps can be called iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T18:11:31.644132Z",
     "start_time": "2018-04-15T17:34:51.706872Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 114890 samples, validate on 12766 samples\n",
      "Epoch 1/2\n",
      "114890/114890 [==============================] - 1078s 9ms/step - loss: 0.0770 - acc: 0.9762 - f1: 0.4786 - val_loss: 0.0502 - val_acc: 0.9816 - val_f1: 0.6600\n",
      "Epoch 2/2\n",
      "114890/114890 [==============================] - 1121s 10ms/step - loss: 0.0461 - acc: 0.9829 - f1: 0.6679 - val_loss: 0.0477 - val_acc: 0.9816 - val_f1: 0.6682\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1298b9668>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 2\n",
    "model.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training model perform over 0.983 in accuracy but has a F1 of 0.67 due to the fact that some labels have a small number of observation. For instance threat label has 478 observations which is really small compare to the 159571 total observation. It will be interesting to add more observations relative to this label to have good recall for our model. At the end, we can see that the model is really good for classifying non toxic comments over the internet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
